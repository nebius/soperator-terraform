clusterName: ${name}

k8sNodeFilters:
  - name: ${k8s_node_filters.non_gpu.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.non_gpu.affinity.key}
                  operator: In
                  values:
                    - ${k8s_node_filters.non_gpu.affinity.value}

  - name: ${k8s_node_filters.gpu.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.gpu.affinity.key}
                  operator: In
                  values:
                    - ${k8s_node_filters.gpu.affinity.value}
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

volumeSources:
  - name: jail
    persistentVolumeClaim:
      claimName: jail-pvc
      readOnly: false

  - name: controller-spool
    persistentVolumeClaim:
      claimName: controller-spool-pvc
      readOnly: false

  - name: worker-spool
    emptyDir:
      sizeLimit: ${nodes.worker.resources.ephemeral_storage}Gi

  %{~ for sub_mount in jail_submounts ~}
  - name: jail-submount-${sub_mount.name}
    persistentVolumeClaim:
      claimName: jail-submount-${sub_mount.name}-pvc
      readOnly: false
  %{~ endfor ~}

populateJail:
  k8sNodeFilterName: ${k8s_node_filters.gpu.name}

periodicChecks:
  ncclBenchmark:
    enabled: false
    k8sNodeFilterName: ${k8s_node_filters.non_gpu.name}
    ncclArguments:
      useInfiniband: ${ncclBenchmark.use_infiniband}

slurmNodes:
  controller:
    size: ${nodes.controller.size}
    k8sNodeFilterName: ${k8s_node_filters.non_gpu.name}

  worker:
    size: ${nodes.worker.size}
    k8sNodeFilterName: ${k8s_node_filters.gpu.name}
    cgroupVersion: v2
    munge:
      resources:
        cpu: 500m
    slurmd:
      resources:
        cpu: ${ceil(nodes.worker.resources.cpu * 1000)}m
        memory: ${nodes.worker.resources.memory}Gi
        ephemeralStorage: ${nodes.worker.resources.ephemeral_storage}Gi
        gpu: ${nodes.worker.resources.gpus}
    volumes:
      spool:
        volumeClaimTemplateSpec: null
        volumeSourceName: worker-spool
      %{~ if length(jail_submounts) > 0 ~}
      jailSubMounts:
        %{~ for sub_mount in jail_submounts ~}
        - name: ${sub_mount.name}
          mountPath: ${sub_mount.mount_path}
          volumeSourceName: jail-submount-${sub_mount.name}
        %{~ endfor ~}
      %{~ endif ~}
      sharedMemorySize: ${nodes.worker.shared_memory}Gi

  login:
    size: ${nodes.login.size}
    k8sNodeFilterName: ${k8s_node_filters.non_gpu.name}
    sshdServiceType: ${nodes.login.service_type}
    %{~ if nodes.login.service_type == "LoadBalancer" ~}
    sshdServiceLoadBalancerIP: "${nodes.login.load_balancer_ip}"
    %{~ endif ~}
    %{~ if nodes.login.service_type == "NodePort" ~}
    sshdServiceNodePort: ${nodes.login.node_port}
    %{~ endif ~}
    %{~ if length(nodes.login.root_public_keys) > 0 ~}
    sshRootPublicKeys:
      %{~ for key in nodes.login.root_public_keys ~}
      - ${key}
      %{~ endfor ~}
    %{~ endif ~}
    %{~ if length(jail_submounts) > 0 ~}
    volumes:
      jailSubMounts:
        %{~ for sub_mount in jail_submounts ~}
        - name: ${sub_mount.name}
          mountPath: ${sub_mount.mount_path}
          volumeSourceName: jail-submount-${sub_mount.name}
        %{~ endfor ~}
      %{~ endif ~}

telemetry:
  jobsTelemetry:
    otelCollectorHttpHost: vmsingle-slurm.monitoring-system.svc.cluster.local
    otelCollectorPath: /opentelemetry/api/v1/push
    otelCollectorPort: 8429
    sendJobsEvents: true
    sendOtelMetrics: true
  openTelemetryCollector:
    enabled: true
    replicasOtelCollector: 1
    otelCollectorPort: 8429
  prometheus:
    enabled: true
