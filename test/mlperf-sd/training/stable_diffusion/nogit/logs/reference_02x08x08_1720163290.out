pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
STARTING TIMING RUN AT 2024-07-05 07:11:17 AM
:::MLLOG {"namespace": "", "time_ms": 1720163482733, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482734, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482734, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482734, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482736, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482744, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482745, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163482744, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163487985, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488003, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163487987, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488004, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163487989, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488005, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488006, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488007, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163488008, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488008, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488009, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488010, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163488012, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488013, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488014, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488015, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488016, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488017, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488004, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488019, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488021, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488022, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488023, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1084932835, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163488023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488025, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488026, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
[rank: 3] Global seed set to 1084932835
:::MLLOG {"namespace": "", "time_ms": 1720163488028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488029, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4156449826, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163488029, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
[rank: 1] Global seed set to 4156449826
:::MLLOG {"namespace": "", "time_ms": 1720163488032, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
Using base config ['./configs/train_02x08x08.yaml']
:::MLLOG {"namespace": "", "time_ms": 1720163488032, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1064920800, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163488033, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
[rank: 0] Global seed set to 1064920800
:::MLLOG {"namespace": "", "time_ms": 1720163488035, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488042, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1697509300, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 5] Global seed set to 1697509300
:::MLLOG {"namespace": "", "time_ms": 1720163488047, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488047, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488064, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488065, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488065, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
Using ckpt_path = /checkpoints/sd/512-base-ema.ckpt
:::MLLOG {"namespace": "", "time_ms": 1720163488066, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163488050, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488067, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488068, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488050, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163488069, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488070, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488071, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488072, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163488072, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488073, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163488074, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488075, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163488076, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488077, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488078, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488079, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163488079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488081, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488084, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163488086, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488087, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2326667607, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163488088, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163488089, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488089, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1164382422, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 6] Global seed set to 2326667607
:::MLLOG {"namespace": "", "time_ms": 1720163488091, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163488092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
[rank: 7] Global seed set to 1164382422
:::MLLOG {"namespace": "", "time_ms": 1720163488093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163488096, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2177684662, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 4] Global seed set to 2177684662
:::MLLOG {"namespace": "", "time_ms": 1720163488096, "event_type": "POINT_IN_TIME", "key": "seed", "value": 662594678, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 2] Global seed set to 662594678
LatentDiffusion: Running in v-prediction mode
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
STARTING TIMING RUN AT 2024-07-05 07:11:29 AM
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
:::MLLOG {"namespace": "", "time_ms": 1720163494878, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494881, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494882, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494881, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494882, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494892, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163494892, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163495000, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1720163500069, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500086, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500087, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500089, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500090, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500091, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500091, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500078, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500095, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1164305260, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163500095, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
[rank: 11] Global seed set to 1164305260
:::MLLOG {"namespace": "", "time_ms": 1720163500096, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500097, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500098, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500099, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500100, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500085, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500102, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500103, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2137920139, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163500104, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
[rank: 14] Global seed set to 2137920139
:::MLLOG {"namespace": "", "time_ms": 1720163500107, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500108, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500109, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500110, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500114, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2015850373, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 9] Global seed set to 2015850373
:::MLLOG {"namespace": "", "time_ms": 1720163500115, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500131, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500133, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500134, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500135, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500136, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500136, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500140, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2422918051, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 12] Global seed set to 2422918051
:::MLLOG {"namespace": "", "time_ms": 1720163500143, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500143, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500160, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500161, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500162, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500147, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500162, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500164, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500165, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500166, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500166, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500168, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500169, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500169, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500170, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500171, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500172, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500172, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500173, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500174, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500176, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500177, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3874400044, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1720163500178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
[rank: 8] Global seed set to 3874400044
:::MLLOG {"namespace": "", "time_ms": 1720163500179, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3416668483, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 10] Global seed set to 3416668483
:::MLLOG {"namespace": "", "time_ms": 1720163500181, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3982661590, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 15] Global seed set to 3982661590
:::MLLOG {"namespace": "", "time_ms": 1720163500192, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1720163500208, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1720163500209, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1720163500210, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1720163500211, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1720163500211, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1720163500212, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1720163500213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1720163500215, "event_type": "POINT_IN_TIME", "key": "seed", "value": 881850193, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 13] Global seed set to 881850193
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526352, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163526356, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526357, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163526358, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526361, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526362, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163526364, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
[rank: 3] Global seed set to 1084932835
:::MLLOG {"namespace": "", "time_ms": 1720163526365, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526365, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/16
:::MLLOG {"namespace": "", "time_ms": 1720163526366, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526367, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526368, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1720163526369, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163526371, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526372, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 5] Global seed set to 1697509300
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/16
[rank: 1] Global seed set to 4156449826
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526496, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163526497, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526498, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163526499, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526500, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 6] Global seed set to 2326667607
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/16
Using 16bit None Automatic Mixed Precision (AMP)
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526506, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163526507, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526508, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163526509, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526510, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526512, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
[rank: 7] Global seed set to 1164382422
:::MLLOG {"namespace": "", "time_ms": 1720163526513, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526513, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
:::MLLOG {"namespace": "", "time_ms": 1720163526515, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163526516, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526516, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526517, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163526517, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163526518, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1720163526519, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163526520, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163526523, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1720163526524, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 4] Global seed set to 2177684662
:::MLLOG {"namespace": "", "time_ms": 1720163526526, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/16
Setting learning rate to 1.60e-05 = 1 (accumulate_grad_batches) * 16 (num_gpus) * 8 (local_batch_size) * 1.25e-07 (base_lr)
:::MLLOG {"namespace": "", "time_ms": 1720163526526, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 2] Global seed set to 662594678
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/16
[rank: 0] Global seed set to 1064920800
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163537907, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163537909, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163537910, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163537911, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163537912, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 13] Global seed set to 881850193
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163537921, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163537922, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163537923, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163537925, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163537926, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163537928, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163537929, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
[rank: 11] Global seed set to 1164305260
:::MLLOG {"namespace": "", "time_ms": 1720163537930, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163537931, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/16
:::MLLOG {"namespace": "", "time_ms": 1720163537932, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 9] Global seed set to 2015850373
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163538006, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163538008, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163538009, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163538010, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163538011, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 12] Global seed set to 2422918051
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163538017, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163538018, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163538020, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163538021, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163538022, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 14] Global seed set to 2137920139
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163538075, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163538076, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163538077, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163538078, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163538079, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 15] Global seed set to 3982661590
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163538095, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163538096, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1720163538097, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1720163538097, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163538098, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163538099, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1720163538100, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1720163538101, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1720163538102, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1720163538103, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 10] Global seed set to 3416668483
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/16
[rank: 8] Global seed set to 3874400044
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/16
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 16 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /results/2024-07-05T07-11-28_train_02x08x08/diff_tb
worker-0:19642:19642 [0] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19642:19642 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19642:19642 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19642:19642 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19642:19642 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19642:19642 [0] NCCL INFO cudaDriverVersion 12030
NCCL version 2.19.3+cuda12.3
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-07-05T07-11-40_train_02x08x08/diff_tb
worker-0:19642:20035 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19642:20035 [0] NCCL INFO P2P plugin IBext
worker-0:19642:20035 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19642:20035 [0] NCCL INFO Using non-device net plugin version 0
worker-0:19642:20035 [0] NCCL INFO Using network IBext
worker-0:19642:20035 [0] NCCL INFO comm 0x558ed3265fa0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x753d166fb39e1659 - Init START
worker-0:19642:20035 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19642:20035 [0] NCCL INFO Setting affinity for GPU 0 to ffff
worker-0:19642:20035 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-0:19642:20035 [0] NCCL INFO NVLS Head  0:  0  8
worker-0:19642:20035 [0] NCCL INFO NVLS Head  1:  1  9
worker-0:19642:20035 [0] NCCL INFO NVLS Head  2:  2 10
worker-0:19642:20035 [0] NCCL INFO NVLS Head  3:  3 11
worker-0:19642:20035 [0] NCCL INFO NVLS Head  4:  4 12
worker-0:19642:20035 [0] NCCL INFO NVLS Head  5:  5 13
worker-0:19642:20035 [0] NCCL INFO NVLS Head  6:  6 14
worker-0:19642:20035 [0] NCCL INFO NVLS Head  7:  7 15
worker-0:19642:20035 [0] NCCL INFO Channel 00/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-0:19642:20035 [0] NCCL INFO Channel 01/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-0:19642:20035 [0] NCCL INFO Channel 02/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 03/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 04/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 05/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 06/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 07/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 08/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-0:19642:20035 [0] NCCL INFO Channel 09/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-0:19642:20035 [0] NCCL INFO Channel 10/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 11/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 12/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 13/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 14/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Channel 15/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-0:19642:20035 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] -1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->7 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7 [8] 1/-1/-1->0->8 [9] -1/-1/-1->0->7 [10] 1/-1/-1->0->7 [11] 1/-1/-1->0->7 [12] 1/-1/-1->0->7 [13] 1/-1/-1->0->7 [14] 1/-1/-1->0->7 [15] 1/-1/-1->0->7
worker-0:19642:20035 [0] NCCL INFO P2P Chunksize set to 131072
worker-0:19642:20035 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19643:19643 [2] NCCL INFO cudaDriverVersion 12030
worker-0:19643:19643 [2] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19643:19643 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19643:19643 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19643:19643 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19643:19643 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19643:20036 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19643:20036 [2] NCCL INFO P2P plugin IBext
worker-0:19643:20036 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19643:20036 [2] NCCL INFO Using non-device net plugin version 0
worker-0:19643:20036 [2] NCCL INFO Using network IBext
worker-0:19643:20036 [2] NCCL INFO comm 0x55c72683f830 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x753d166fb39e1659 - Init START
worker-0:19643:20036 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19643:20036 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
worker-0:19643:20036 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-0:19643:20036 [2] NCCL INFO NVLS Head  0:  0  8
worker-0:19643:20036 [2] NCCL INFO NVLS Head  1:  1  9
worker-0:19643:20036 [2] NCCL INFO NVLS Head  2:  2 10
worker-0:19643:20036 [2] NCCL INFO NVLS Head  3:  3 11
worker-0:19643:20036 [2] NCCL INFO NVLS Head  4:  4 12
worker-0:19643:20036 [2] NCCL INFO NVLS Head  5:  5 13
worker-0:19643:20036 [2] NCCL INFO NVLS Head  6:  6 14
worker-0:19643:20036 [2] NCCL INFO NVLS Head  7:  7 15
worker-0:19643:20036 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/10/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->10 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
worker-0:19643:20036 [2] NCCL INFO P2P Chunksize set to 131072
worker-0:19643:20036 [2] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:19437:19437 [2] NCCL INFO cudaDriverVersion 12030
worker-1:19437:19437 [2] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19437:19437 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19437:19437 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19437:19437 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19437:19437 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19437:19859 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19437:19859 [2] NCCL INFO P2P plugin IBext
worker-1:19437:19859 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19437:19859 [2] NCCL INFO Using non-device net plugin version 0
worker-1:19437:19859 [2] NCCL INFO Using network IBext
worker-0:19643:20036 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Connected all rings
worker-0:19643:20036 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO comm 0x5635fc351070 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x753d166fb39e1659 - Init START
worker-1:19437:19859 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19437:19859 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
worker-1:19437:19859 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-1:19437:19859 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->2 [3] -1/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9 [8] 11/-1/-1->10->9 [9] 11/-1/-1->10->9 [10] 11/2/-1->10->-1 [11] -1/-1/-1->10->9 [12] 11/-1/-1->10->9 [13] 11/-1/-1->10->9 [14] 11/-1/-1->10->9 [15] 11/-1/-1->10->9
worker-1:19437:19859 [2] NCCL INFO P2P Chunksize set to 131072
worker-1:19437:19859 [2] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 08/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 10/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:19592:19592 [1] NCCL INFO cudaDriverVersion 12030
worker-0:19592:19592 [1] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19592:19592 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19592:19592 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19592:19592 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19592:19592 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19592:20041 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19592:20041 [1] NCCL INFO P2P plugin IBext
worker-0:19592:20041 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19592:20041 [1] NCCL INFO Using non-device net plugin version 0
worker-0:19592:20041 [1] NCCL INFO Using network IBext
worker-1:19437:19859 [2] NCCL INFO Channel 11/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 12/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 13/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 14/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 15/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Connected all rings
worker-1:19437:19859 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO comm 0x5568409743a0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x753d166fb39e1659 - Init START
worker-0:19592:20041 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19592:20041 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-0:19592:20041 [1] NCCL INFO NVLS Head  0:  0  8
worker-0:19592:20041 [1] NCCL INFO NVLS Head  1:  1  9
worker-0:19592:20041 [1] NCCL INFO NVLS Head  2:  2 10
worker-0:19592:20041 [1] NCCL INFO NVLS Head  3:  3 11
worker-0:19592:20041 [1] NCCL INFO NVLS Head  4:  4 12
worker-0:19592:20041 [1] NCCL INFO NVLS Head  5:  5 13
worker-0:19592:20041 [1] NCCL INFO NVLS Head  6:  6 14
worker-0:19592:20041 [1] NCCL INFO NVLS Head  7:  7 15
worker-1:19437:19859 [2] NCCL INFO Channel 08/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/9/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->9 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
worker-0:19592:20041 [1] NCCL INFO P2P Chunksize set to 131072
worker-0:19592:20041 [1] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:19435:19435 [0] NCCL INFO cudaDriverVersion 12030
worker-1:19435:19435 [0] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19435:19435 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19435:19435 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19435:19435 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19435:19435 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19435:19857 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19435:19857 [0] NCCL INFO P2P plugin IBext
worker-1:19435:19857 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19435:19857 [0] NCCL INFO Using non-device net plugin version 0
worker-1:19435:19857 [0] NCCL INFO Using network IBext
worker-0:19592:20041 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Connected all rings
worker-0:19592:20041 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO comm 0x5555845fb550 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x753d166fb39e1659 - Init START
worker-1:19435:19857 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19435:19857 [0] NCCL INFO Setting affinity for GPU 0 to ffff
worker-1:19435:19857 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-1:19435:19857 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] -1/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/-1/-1->8->15 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15 [8] 9/0/-1->8->-1 [9] -1/-1/-1->8->15 [10] 9/-1/-1->8->15 [11] 9/-1/-1->8->15 [12] 9/-1/-1->8->15 [13] 9/-1/-1->8->15 [14] 9/-1/-1->8->15 [15] 9/-1/-1->8->15
worker-1:19435:19857 [0] NCCL INFO P2P Chunksize set to 131072
worker-1:19435:19857 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 08/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 09/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 10/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 11/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 12/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 13/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:19591:19591 [5] NCCL INFO cudaDriverVersion 12030
worker-0:19591:19591 [5] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19591:19591 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19591:19591 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19591:19591 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19591:19591 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19591:20038 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19591:20038 [5] NCCL INFO P2P plugin IBext
worker-0:19591:20038 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19591:20038 [5] NCCL INFO Using non-device net plugin version 0
worker-0:19591:20038 [5] NCCL INFO Using network IBext
worker-1:19435:19857 [0] NCCL INFO Channel 14/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Connected all rings
worker-1:19435:19857 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 08/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 10/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO comm 0x55d74a8ed200 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x753d166fb39e1659 - Init START
worker-0:19591:20038 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19591:20038 [5] NCCL INFO Setting affinity for GPU 5 to ffff0000,00000000,00000000,00000000
worker-0:19591:20038 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-0:19591:20038 [5] NCCL INFO NVLS Head  0:  0  8
worker-0:19591:20038 [5] NCCL INFO NVLS Head  1:  1  9
worker-0:19591:20038 [5] NCCL INFO NVLS Head  2:  2 10
worker-0:19591:20038 [5] NCCL INFO NVLS Head  3:  3 11
worker-0:19591:20038 [5] NCCL INFO NVLS Head  4:  4 12
worker-0:19591:20038 [5] NCCL INFO NVLS Head  5:  5 13
worker-0:19591:20038 [5] NCCL INFO NVLS Head  6:  6 14
worker-0:19591:20038 [5] NCCL INFO NVLS Head  7:  7 15
worker-1:19401:19401 [1] NCCL INFO cudaDriverVersion 12030
worker-1:19401:19401 [1] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19401:19401 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19401:19401 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19401:19401 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19401:19401 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19401:19858 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19401:19858 [1] NCCL INFO P2P plugin IBext
worker-1:19401:19858 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19401:19858 [1] NCCL INFO Using non-device net plugin version 0
worker-1:19401:19858 [1] NCCL INFO Using network IBext
worker-0:19591:20038 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/13/-1->5->-1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->13 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4
worker-0:19591:20038 [5] NCCL INFO P2P Chunksize set to 131072
worker-0:19591:20038 [5] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO comm 0x55945920c340 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x753d166fb39e1659 - Init START
worker-1:19401:19858 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19401:19858 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-1:19401:19858 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->1 [2] -1/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8 [8] 10/-1/-1->9->8 [9] 10/1/-1->9->-1 [10] -1/-1/-1->9->8 [11] 10/-1/-1->9->8 [12] 10/-1/-1->9->8 [13] 10/-1/-1->9->8 [14] 10/-1/-1->9->8 [15] 10/-1/-1->9->8
worker-1:19401:19858 [1] NCCL INFO P2P Chunksize set to 131072
worker-1:19401:19858 [1] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Connected all rings
worker-0:19591:20038 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 09/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 10/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 11/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 12/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 13/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 14/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 15/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Connected all rings
worker-1:19401:19858 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 08/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:19641:19641 [6] NCCL INFO cudaDriverVersion 12030
worker-0:19641:19641 [6] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19641:19641 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19641:19641 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19641:19641 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19641:19641 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19641:20037 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19641:20037 [6] NCCL INFO P2P plugin IBext
worker-0:19641:20037 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19641:20037 [6] NCCL INFO Using non-device net plugin version 0
worker-0:19641:20037 [6] NCCL INFO Using network IBext
worker-1:19401:19858 [1] NCCL INFO Channel 09/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 11/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO comm 0x564cf01143f0 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x753d166fb39e1659 - Init START
worker-0:19641:20037 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19641:20037 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-0:19641:20037 [6] NCCL INFO NVLS Head  0:  0  8
worker-0:19641:20037 [6] NCCL INFO NVLS Head  1:  1  9
worker-0:19641:20037 [6] NCCL INFO NVLS Head  2:  2 10
worker-0:19641:20037 [6] NCCL INFO NVLS Head  3:  3 11
worker-0:19641:20037 [6] NCCL INFO NVLS Head  4:  4 12
worker-0:19641:20037 [6] NCCL INFO NVLS Head  5:  5 13
worker-0:19641:20037 [6] NCCL INFO NVLS Head  6:  6 14
worker-0:19641:20037 [6] NCCL INFO NVLS Head  7:  7 15
worker-1:19433:19433 [6] NCCL INFO cudaDriverVersion 12030
worker-1:19433:19433 [6] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19433:19433 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19433:19433 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19433:19433 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19433:19433 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19433:19855 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19433:19855 [6] NCCL INFO P2P plugin IBext
worker-1:19433:19855 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19433:19855 [6] NCCL INFO Using non-device net plugin version 0
worker-1:19433:19855 [6] NCCL INFO Using network IBext
worker-0:19641:20037 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/14/-1->6->-1 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->14 [15] -1/-1/-1->6->5
worker-0:19641:20037 [6] NCCL INFO P2P Chunksize set to 131072
worker-0:19641:20037 [6] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO comm 0x5638f36447c0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x753d166fb39e1659 - Init START
worker-1:19433:19855 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19433:19855 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-1:19433:19855 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->6 [7] -1/-1/-1->14->13 [8] 15/-1/-1->14->13 [9] 15/-1/-1->14->13 [10] 15/-1/-1->14->13 [11] 15/-1/-1->14->13 [12] 15/-1/-1->14->13 [13] 15/-1/-1->14->13 [14] 15/6/-1->14->-1 [15] -1/-1/-1->14->13
worker-1:19433:19855 [6] NCCL INFO P2P Chunksize set to 131072
worker-1:19433:19855 [6] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Connected all rings
worker-0:19641:20037 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 03/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 06/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 07/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 08/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 09/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 10/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 11/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 12/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 14/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 15/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Connected all rings
worker-1:19433:19855 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 08/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:19593:19593 [3] NCCL INFO cudaDriverVersion 12030
worker-0:19593:19593 [3] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19593:19593 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19593:19593 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19593:19593 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19593:19593 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19593:20042 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19593:20042 [3] NCCL INFO P2P plugin IBext
worker-0:19593:20042 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19593:20042 [3] NCCL INFO Using non-device net plugin version 0
worker-0:19593:20042 [3] NCCL INFO Using network IBext
worker-1:19433:19855 [6] NCCL INFO Channel 09/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO comm 0x55d0e1ef3e70 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x753d166fb39e1659 - Init START
worker-0:19593:20042 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19593:20042 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-0:19593:20042 [3] NCCL INFO NVLS Head  0:  0  8
worker-0:19593:20042 [3] NCCL INFO NVLS Head  1:  1  9
worker-0:19593:20042 [3] NCCL INFO NVLS Head  2:  2 10
worker-0:19593:20042 [3] NCCL INFO NVLS Head  3:  3 11
worker-0:19593:20042 [3] NCCL INFO NVLS Head  4:  4 12
worker-0:19593:20042 [3] NCCL INFO NVLS Head  5:  5 13
worker-0:19593:20042 [3] NCCL INFO NVLS Head  6:  6 14
worker-0:19593:20042 [3] NCCL INFO NVLS Head  7:  7 15
worker-1:19513:19513 [5] NCCL INFO cudaDriverVersion 12030
worker-1:19513:19513 [5] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19513:19513 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19513:19513 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19513:19513 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19513:19513 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19513:19856 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19513:19856 [5] NCCL INFO P2P plugin IBext
worker-1:19513:19856 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19513:19856 [5] NCCL INFO Using non-device net plugin version 0
worker-1:19513:19856 [5] NCCL INFO Using network IBext
worker-0:19593:20042 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/11/-1->3->-1 [4] -1/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->11 [12] -1/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
worker-0:19593:20042 [3] NCCL INFO P2P Chunksize set to 131072
worker-0:19593:20042 [3] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO comm 0x55b59b6ede30 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x753d166fb39e1659 - Init START
worker-1:19513:19856 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19513:19856 [5] NCCL INFO Setting affinity for GPU 5 to ffff0000,00000000,00000000,00000000
worker-1:19513:19856 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-1:19513:19856 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->5 [6] -1/-1/-1->13->12 [7] 14/-1/-1->13->12 [8] 14/-1/-1->13->12 [9] 14/-1/-1->13->12 [10] 14/-1/-1->13->12 [11] 14/-1/-1->13->12 [12] 14/-1/-1->13->12 [13] 14/5/-1->13->-1 [14] -1/-1/-1->13->12 [15] 14/-1/-1->13->12
worker-1:19513:19856 [5] NCCL INFO P2P Chunksize set to 131072
worker-1:19513:19856 [5] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Connected all rings
worker-0:19593:20042 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 05/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 08/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 09/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 10/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 11/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 13/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 14/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 15/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Connected all rings
worker-1:19513:19856 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 03/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 07/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:19640:19640 [4] NCCL INFO cudaDriverVersion 12030
worker-0:19640:19640 [4] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19640:19640 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19640:19640 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19640:19640 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19640:19640 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19640:20039 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19640:20039 [4] NCCL INFO P2P plugin IBext
worker-0:19640:20039 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19640:20039 [4] NCCL INFO Using non-device net plugin version 0
worker-0:19640:20039 [4] NCCL INFO Using network IBext
worker-1:19513:19856 [5] NCCL INFO Channel 08/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO comm 0x556e1f7c97c0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x753d166fb39e1659 - Init START
worker-0:19640:20039 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19640:20039 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-0:19640:20039 [4] NCCL INFO NVLS Head  0:  0  8
worker-0:19640:20039 [4] NCCL INFO NVLS Head  1:  1  9
worker-0:19640:20039 [4] NCCL INFO NVLS Head  2:  2 10
worker-0:19640:20039 [4] NCCL INFO NVLS Head  3:  3 11
worker-0:19640:20039 [4] NCCL INFO NVLS Head  4:  4 12
worker-0:19640:20039 [4] NCCL INFO NVLS Head  5:  5 13
worker-0:19640:20039 [4] NCCL INFO NVLS Head  6:  6 14
worker-0:19640:20039 [4] NCCL INFO NVLS Head  7:  7 15
worker-1:19402:19402 [3] NCCL INFO cudaDriverVersion 12030
worker-1:19402:19402 [3] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19402:19402 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19402:19402 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19402:19402 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19402:19402 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19402:19853 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19402:19853 [3] NCCL INFO P2P plugin IBext
worker-1:19402:19853 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19402:19853 [3] NCCL INFO Using non-device net plugin version 0
worker-1:19402:19853 [3] NCCL INFO Using network IBext
worker-0:19640:20039 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/12/-1->4->-1 [5] -1/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->12 [13] -1/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
worker-0:19640:20039 [4] NCCL INFO P2P Chunksize set to 131072
worker-0:19640:20039 [4] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO comm 0x5648f2420a90 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x753d166fb39e1659 - Init START
worker-1:19402:19853 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19402:19853 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-1:19402:19853 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->3 [4] -1/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] 12/-1/-1->11->10 [7] 12/-1/-1->11->10 [8] 12/-1/-1->11->10 [9] 12/-1/-1->11->10 [10] 12/-1/-1->11->10 [11] 12/3/-1->11->-1 [12] -1/-1/-1->11->10 [13] 12/-1/-1->11->10 [14] 12/-1/-1->11->10 [15] 12/-1/-1->11->10
worker-1:19402:19853 [3] NCCL INFO P2P Chunksize set to 131072
worker-1:19402:19853 [3] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Connected all rings
worker-0:19640:20039 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 08/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 09/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 11/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 12/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 13/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 14/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 15/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Connected all rings
worker-1:19402:19853 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 06/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 08/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:19639:19639 [7] NCCL INFO cudaDriverVersion 12030
worker-0:19639:19639 [7] NCCL INFO Bootstrap : Using eth0:172.17.3.106<0>
worker-0:19639:19639 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:19639:19639 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:19639:19639 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:19639:19639 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:19639:20040 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:19639:20040 [7] NCCL INFO P2P plugin IBext
worker-0:19639:20040 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.3.106<0>
worker-0:19639:20040 [7] NCCL INFO Using non-device net plugin version 0
worker-0:19639:20040 [7] NCCL INFO Using network IBext
worker-1:19402:19853 [3] NCCL INFO Channel 09/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO comm 0x56319fd69600 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x753d166fb39e1659 - Init START
worker-0:19639:20040 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:19639:20040 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-0:19639:20040 [7] NCCL INFO NVLS Head  0:  0  8
worker-0:19639:20040 [7] NCCL INFO NVLS Head  1:  1  9
worker-0:19639:20040 [7] NCCL INFO NVLS Head  2:  2 10
worker-0:19639:20040 [7] NCCL INFO NVLS Head  3:  3 11
worker-0:19639:20040 [7] NCCL INFO NVLS Head  4:  4 12
worker-0:19639:20040 [7] NCCL INFO NVLS Head  5:  5 13
worker-0:19639:20040 [7] NCCL INFO NVLS Head  6:  6 14
worker-0:19639:20040 [7] NCCL INFO NVLS Head  7:  7 15
worker-1:19434:19434 [4] NCCL INFO cudaDriverVersion 12030
worker-1:19434:19434 [4] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19434:19434 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19434:19434 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19434:19434 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19434:19434 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19434:19854 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19434:19854 [4] NCCL INFO P2P plugin IBext
worker-1:19434:19854 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19434:19854 [4] NCCL INFO Using non-device net plugin version 0
worker-1:19434:19854 [4] NCCL INFO Using network IBext
worker-0:19639:20040 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] 0/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/15/-1->7->-1 [8] -1/-1/-1->7->6 [9] 0/-1/-1->7->6 [10] 0/-1/-1->7->6 [11] 0/-1/-1->7->6 [12] 0/-1/-1->7->6 [13] 0/-1/-1->7->6 [14] 0/-1/-1->7->6 [15] 0/-1/-1->7->15
worker-0:19639:20040 [7] NCCL INFO P2P Chunksize set to 131072
worker-0:19639:20040 [7] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO comm 0x559d3654a610 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x753d166fb39e1659 - Init START
worker-1:19434:19854 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19434:19854 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-1:19434:19854 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->4 [5] -1/-1/-1->12->11 [6] 13/-1/-1->12->11 [7] 13/-1/-1->12->11 [8] 13/-1/-1->12->11 [9] 13/-1/-1->12->11 [10] 13/-1/-1->12->11 [11] 13/-1/-1->12->11 [12] 13/4/-1->12->-1 [13] -1/-1/-1->12->11 [14] 13/-1/-1->12->11 [15] 13/-1/-1->12->11
worker-1:19434:19854 [4] NCCL INFO P2P Chunksize set to 131072
worker-1:19434:19854 [4] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Connected all rings
worker-0:19639:20040 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 08/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 09/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 10/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 12/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 13/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 14/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 15/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Connected all rings
worker-1:19434:19854 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 08/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 09/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19436:19436 [7] NCCL INFO cudaDriverVersion 12030
worker-1:19436:19436 [7] NCCL INFO Bootstrap : Using eth0:172.17.2.189<0>
worker-1:19436:19436 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:19436:19436 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:19436:19436 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:19436:19436 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:19436:19860 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:19436:19860 [7] NCCL INFO P2P plugin IBext
worker-1:19436:19860 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:172.17.2.189<0>
worker-1:19436:19860 [7] NCCL INFO Using non-device net plugin version 0
worker-1:19436:19860 [7] NCCL INFO Using network IBext
worker-1:19436:19860 [7] NCCL INFO comm 0x55831b79cb50 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x753d166fb39e1659 - Init START
worker-1:19436:19860 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:19436:19860 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-1:19436:19860 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] 8/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->7 [8] -1/-1/-1->15->14 [9] 8/-1/-1->15->14 [10] 8/-1/-1->15->14 [11] 8/-1/-1->15->14 [12] 8/-1/-1->15->14 [13] 8/-1/-1->15->14 [14] 8/-1/-1->15->14 [15] 8/7/-1->15->-1
worker-1:19436:19860 [7] NCCL INFO P2P Chunksize set to 131072
worker-1:19436:19860 [7] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 07/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 08/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 09/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 10/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 11/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 12/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 13/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 15/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Connected all rings
worker-1:19436:19860 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 04/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 08/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 11/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 12/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 13/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 14/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Connected all rings
worker-0:19642:20035 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:19642:20035 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Connected all trees
worker-0:19642:20035 [0] NCCL INFO NVLS comm 0x558ed3265fa0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19642:20035 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:19591:20038 [5] NCCL INFO Connected all trees
worker-0:19591:20038 [5] NCCL INFO NVLS comm 0x55d74a8ed200 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19591:20038 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:19639:20040 [7] NCCL INFO Connected all trees
worker-0:19639:20040 [7] NCCL INFO NVLS comm 0x56319fd69600 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19639:20040 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:19640:20039 [4] NCCL INFO Connected all trees
worker-0:19640:20039 [4] NCCL INFO NVLS comm 0x556e1f7c97c0 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19640:20039 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:19641:20037 [6] NCCL INFO Connected all trees
worker-0:19641:20037 [6] NCCL INFO NVLS comm 0x564cf01143f0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19641:20037 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Connected all trees
worker-0:19593:20042 [3] NCCL INFO NVLS comm 0x55d0e1ef3e70 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19593:20042 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 09/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 10/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 11/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 12/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 13/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 15/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:19513:19856 [5] NCCL INFO Channel 12/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:19593:20042 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Connected all trees
worker-1:19513:19856 [5] NCCL INFO NVLS comm 0x55b59b6ede30 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19513:19856 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 14/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-1:19513:19856 [5] NCCL INFO Connected NVLS tree
worker-0:19641:20037 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:19513:19856 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19593:20042 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:19643:20036 [2] NCCL INFO Connected all trees
worker-0:19643:20036 [2] NCCL INFO NVLS comm 0x55c72683f830 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19643:20036 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 11/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 12/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 13/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 14/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 15/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 15/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:19435:19857 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Connected all trees
worker-0:19643:20036 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19435:19857 [0] NCCL INFO NVLS comm 0x5555845fb550 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19435:19857 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 14/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:19435:19857 [0] NCCL INFO Connected NVLS tree
worker-0:19592:20041 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:19435:19857 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19435:19857 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19592:20041 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Connected all trees
worker-0:19592:20041 [1] NCCL INFO NVLS comm 0x5568409743a0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:19592:20041 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 09/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 10/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 12/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 13/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 14/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 15/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:19437:19859 [2] NCCL INFO Channel 09/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:19592:20041 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
Configure sharded model for LatentDiffusion
worker-1:19437:19859 [2] NCCL INFO Connected all trees
worker-1:19437:19859 [2] NCCL INFO NVLS comm 0x5635fc351070 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19437:19859 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-0:19592:20041 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-1:19437:19859 [2] NCCL INFO Connected NVLS tree
worker-1:19437:19859 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19434:19854 [4] NCCL INFO Channel 10/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 11/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 12/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 14/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 15/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Channel 11/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:19434:19854 [4] NCCL INFO Connected all trees
worker-1:19434:19854 [4] NCCL INFO NVLS comm 0x559d3654a610 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19434:19854 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-1:19434:19854 [4] NCCL INFO Connected NVLS tree
worker-1:19434:19854 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19434:19854 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:19433:19855 [6] NCCL INFO Channel 10/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 11/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 12/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 13/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 14/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Channel 13/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:19433:19855 [6] NCCL INFO Connected all trees
worker-1:19433:19855 [6] NCCL INFO NVLS comm 0x5638f36447c0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19433:19855 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-1:19433:19855 [6] NCCL INFO Connected NVLS tree
worker-1:19433:19855 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19433:19855 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:19436:19860 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 09/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 10/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 11/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 12/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 13/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 14/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 15/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Channel 14/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-1:19436:19860 [7] NCCL INFO Connected all trees
worker-1:19436:19860 [7] NCCL INFO NVLS comm 0x55831b79cb50 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19436:19860 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-1:19436:19860 [7] NCCL INFO Connected NVLS tree
worker-1:19436:19860 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19436:19860 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:19437:19859 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:19402:19853 [3] NCCL INFO Channel 10/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 11/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 13/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 14/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 15/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Channel 10/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:19402:19853 [3] NCCL INFO Connected all trees
worker-1:19402:19853 [3] NCCL INFO NVLS comm 0x5648f2420a90 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19402:19853 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:19402:19853 [3] NCCL INFO Connected NVLS tree
worker-1:19402:19853 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19402:19853 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
Deleting key model.diffusion_model.out.0.bias from state_dict.
Deleting key model.diffusion_model.out.2.weight from state_dict.
Deleting key model.diffusion_model.out.2.bias from state_dict.
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
Missing Keys:
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
Deleting key model.diffusion_model.out.0.bias from state_dict.
Deleting key model.diffusion_model.out.2.weight from state_dict.
Deleting key model.diffusion_model.out.2.bias from state_dict.
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
Missing Keys:
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
worker-0:19642:20035 [0] NCCL INFO Channel 14/0 : 8[0] Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
:::MLLOG {"namespace": "", "time_ms": 1720163658109, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1639}}
:::MLLOG {"namespace": "", "time_ms": 1720163658157, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1720163658159, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1641}}
:::MLLOG {"namespace": "", "time_ms": 1720163658160, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1642}}
:::MLLOG {"namespace": "", "time_ms": 1720163658160, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1643}}
:::MLLOG {"namespace": "", "time_ms": 1720163658161, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.6e-05, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1644}}
:::MLLOG {"namespace": "", "time_ms": 1720163658162, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1650}}
Setting up LambdaLR scheduler...
Project config
model:
  base_learning_rate: 1.25e-07
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    parameterization: v
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: npy
    first_stage_type: moments
    cond_stage_key: txt
    image_size: 64
    channels: 4
    cond_stage_trainable: false
    conditioning_key: crossattn
    monitor: steps
    scale_factor: 0.18215
    use_ema: false
    load_vae: true
    load_unet: false
    load_encoder: true
    validation_config:
      sampler: ddim
      steps: 50
      scale: 8.0
      ddim_eta: 0.0
      prompt_key: caption
      image_fname_key: image_id
      save_images:
        enabled: false
        base_output_dir: /results/inference
      fid:
        enabled: true
        inception_weights_url: https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth
        cache_dir: /checkpoints/inception
        gt_path: /datasets/coco2014/val2014_30k_stats.npz
      clip:
        enabled: true
        clip_version: ViT-H-14
        cache_dir: /checkpoints/clip
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 1000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        use_checkpoint: false
        use_fp16: true
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        num_head_channels: 64
        use_spatial_transformer: true
        use_linear_in_transformer: true
        transformer_depth: 1
        context_dim: 1024
        legacy: false
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder
      params:
        arch: ViT-H-14
        version: laion2b_s32b_b79k
        freeze: true
        layer: penultimate
        cache_dir: /checkpoints/clip
    use_fp16: true
    ckpt: /checkpoints/sd/512-base-ema.ckpt
data:
  target: ldm.data.composable_data_module.ComposableDataModule
  params:
    train:
      target: ldm.data.webdatasets.build_dataloader
      params:
        urls: /datasets/laion-400m/webdataset-moments-filtered/{00000..00831}.tar
        batch_size: 8
        shuffle: 1000
        partial: false
        keep_only_keys:
        - npy
        - txt
        num_workers: 4
        persistent_workers: true
    validation:
      target: ldm.data.tsv.build_dataloader
      params:
        annotations_file: /datasets/coco2014/val2014_30k.tsv
        keys:
        - image_id
        - id
        - caption
        batch_size: 8
        shuffle: false
        num_workers: 1

Lightning config
trainer:
  accelerator: gpu
  num_nodes: 2
  devices: 8
  precision: 16
  logger: false
  log_every_n_steps: 5
  enable_progress_bar: false
  max_epochs: -1
  max_steps: 10000000000000
  val_check_interval: 1000
  enable_checkpointing: true
  num_sanity_val_steps: 0
  strategy:
    target: strategies.DDPStrategy
    params:
      find_unused_parameters: false
modelcheckpoint:
  target: lightning.pytorch.callbacks.ModelCheckpoint
  params:
    save_top_k: -1
    every_n_train_steps: 1000

:::MLLOG {"namespace": "", "time_ms": 1720163658202, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 88}}
SLURM auto-requeueing enabled. Setting signal handlers.

  | Name              | Type                   | Params
-------------------------------------------------------------
0 | model             | DiffusionWrapper       | 865 M 
1 | first_stage_model | AutoencoderKL          | 83.7 M
2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
-------------------------------------------------------------
865 M     Trainable params
437 M     Non-trainable params
1.3 B     Total params
2,607.194 Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.
  rank_zero_warn(
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
worker-1:19435:19857 [0] NCCL INFO comm 0x5555845fb550 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8dSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
worker-0:19643:20036 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
000 commId 0x753d166fb39e1659 - Init COMPLETE
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
worker-1:19434:19854 [4] NCCL INFO comm 0x559d3654a610 rankSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
SLURM auto-requeueing enabled. Setting signal handlers.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:19640:20039 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-1:19401:19858 [1] NCCL INFO CSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
RDMA
worker-0:19643:20036 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-0:19643:20036 [2] NCCL INFO Connected NVLS tree
worker-0:19643:20036 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19643:20036 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19643:20036 [2] NCCL INFO comm 0x55c72683f830 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x753d166fb39e1659 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
nd] via NET/IBext/0/GDRDMA
worker-0:19640:20039 [4] NCCL INFO Connected NVLS tree
worker-0:19640:20039 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19640:20039 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19640:20039 [4] NCCL INFO comm 0x556e1f7c97c0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x753d166fb39e1659 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:19593:20042 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-1:19437:19Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:19641:20037 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-1:19436:19860 [7] NCCL INFO comm 0x55831b79cb50 rank 15 nranks 16 cudaDev 7 nvmlSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 160 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
worker-0:Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Training is starting
worker-1:19433:19855 [6] NCCL INFO comm 0x5638f36447c0 rankSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
nd] via NET/IBext/7/GDRDMA
worker-0:19593:20042 [3] NCCL INFO Connected NVLS tree
worker-0:19593:20042 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19593:20042 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19593:20042 [3] NCCL INFO comm 0x55d0e1ef3e70 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x753d166fb39e1659 - Init COMPLETE
worker-1:19402:19853 [3] NCCL INFO comm 0x5648f2420a90 rankSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:19591:20038 [5] NCCL INFO Channel 14/0 : 5[5] -> 13Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-1:19513:19856 [5] NCCL INFO 16 coll channels, 16 nvls channels,Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:19639:20040 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [sendSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
859 [2] NCCL INFO comm 0x5635fc351070 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x753d166fb39e1659 - Init COMPLETE
nd] via NET/IBext/2/GDRDMA
worker-0:19641:20037 [6] NCCL INFO Connected NVLS tree
worker-0:19641:20037 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19641:20037 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19641:20037 [6] NCCL INFO comm 0x564cf01143f0 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x753d166fb39e1659 - Init COMPLETE
Dev 7 busId b7000 commId 0x753d166fb39e1659 - Init COMPLETE
19592:20041 [1] NCCL INFO Connected NVLS tree
worker-0:19592:20041 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19592:20041 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19592:20041 [1] NCCL INFO comm 0x5568409743a0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x753d166fb39e1659 - Init COMPLETE
 12 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x753d166fb39e1659 - Init COMPLETE
] via NET/IBext/3/GDRDMA
worker-0:19639:20040 [7] NCCL INFO Connected NVLS tree
worker-0:19639:20040 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19639:20040 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19639:20040 [7] NCCL INFO comm 0x56319fd69600 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x753d166fb39e1659 - Init COMPLETE
 14 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x753d166fb39e1659 - Init COMPLETE
[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-0:19591:20038 [5] NCCL INFO Connected NVLS tree
worker-0:19591:20038 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19591:20038 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19591:20038 [5] NCCL INFO comm 0x55d74a8ed200 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x753d166fb39e1659 - Init COMPLETE
 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x753d166fb39e1659 - Init COMPLETE
 16 p2p channels, 2 p2p channels per peer
worker-1:19513:19856 [5] NCCL INFO comm 0x55b59b6ede30 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x753d166fb39e1659 - Init COMPLETE
hannel 12/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 13/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 14/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 15/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Channel 08/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:19401:19858 [1] NCCL INFO Connected all trees
-> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:19401:19858 [1] NCCL INFO NVLS comm 0x55945920c340 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:19401:19858 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-0:19642:20035 [0] NCCL INFO Connected NVLS tree
worker-0:19642:20035 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:19642:20035 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:19642:20035 [0] NCCL INFO comm 0x558ed3265fa0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x753d166fb39e1659 - Init COMPLETE
worker-1:19401:19858 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:19401:19858 [1] NCCL INFO Connected NVLS tree
worker-1:19401:19858 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:19401:19858 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:19401:19858 [1] NCCL INFO comm 0x55945920c340 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x753d166fb39e1659 - Init COMPLETE
:::MLLOG {"namespace": "", "time_ms": 1720163659077, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 0}}
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
:::MLLOG {"namespace": "", "time_ms": 1720163682690, "event_type": "POINT_IN_TIME", "key": "loss", "value": 1.1063201427459717, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1720163682694, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.5840144159999998e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1720163682695, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1720163682717, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1720163703248, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.7823864817619324, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1720163703251, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 3.1840128159999994e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1720163703253, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1720163703279, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1720163723812, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.7460034489631653, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1720163723816, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 4.784011215999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1720163723817, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1720163723837, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1720163743891, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.6784582138061523, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1720163743894, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 6.384009615999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1720163743896, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1720163743923, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1720163764197, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.6897756457328796, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1720163764200, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 7.984008015999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1720163764205, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1720163764225, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1720163784186, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.46772491931915283, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1720163784190, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 9.584006416e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1720163784191, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1720163784211, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1720163804219, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.5544390678405762, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1720163804222, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.1184004815999999e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1720163804223, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1720163804246, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1720163824241, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4802115857601166, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1720163824244, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.2784003215999998e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1720163824245, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1720163824267, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1720163844429, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.5013718605041504, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1720163844432, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.4384001615999999e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1720163844433, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1720163844453, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1720163864394, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4878907799720764, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1720163864397, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.5984000016e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1720163864398, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1720164021447, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 119}}
:::MLLOG {"namespace": "", "time_ms": 1720164021602, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1720164060119, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1720164094414, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1720164098228, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1720164132492, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1720164136281, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 20}}
